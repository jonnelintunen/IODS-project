## Regression and model validation (week 2)

Dataset is read from the local folder:
```{r}
getwd()
setwd("/Users/jonnelintunen/Desktop/IODS-project/data")
table1 <- read.table("learning2014.txt")
head(table1)
```
Dataset has 166 observations (= individuals) and 7 variables (**gender**, **age**, global **attitude** towards statistics, means of the questions relating to **deep**, **strategic** and **surface** learning and total **points** of the survey).

### Overview of the dataset
```{r}
library(ggplot2)
library(plyr)
```

**Gender variable**
```{r}
gender_F <- subset(table1, gender == "F")
nrow(gender_F)
gender_M <- subset(table1, gender == "M")
nrow(gender_M)

gender_counter <- count(table1, "gender")
barplot(gender_counter$freq, main = "Gender distribution", xlab = "Gender", ylab = "No.", col = c("pink", "lightblue"), names.arg = c("Female", "Male"), ylim = c(0, 120))

```

There are 110 females and 56 males in this dataset.

**Age variable**
```{r}
age_all <- table1$age
mean(age_all)
```
Mean age of the dataset is 25.51 years.
```{r}
age_F <- subset(table1, gender == "F")
mean(age_F$age)
min(age_F$age)
max(age_F$age)
```
Mean age of females is 24.85 years; the youngest is 17 years and the oldest 53 years.
```{r}
hist(age_F$age, main = "Age Distribution of Females", xlab = "Age", ylab = "No.", ylim = c(0, 60), xlim = c(10, 60), col = "pink")
```

```{r}
age_M <- subset(table1, gender == "M")
mean(age_M$age)
min(age_M$age)
max(age_M$age)
```
Mean age of males is 26.80 years; the youngest is 19 years and the oldest is 55 years.
```{r}
hist(age_M$age, main = "Age Distribution of Males", xlab = "Age", ylab = "No.", ylim = c(0, 60), xlim = c(10, 60), col = "lightblue")
```

### Regression model  

```{r}
library(GGally)
ggpairs(table1)
```

The target (dependent) variable is **points**. Explanatory variables that I chose are **attitude**, **stra** and **deep** since they have the highest correlations.
```{r}
my_model <- lm(points ~ attitude  + stra + deep, data = table1)
summary(my_model)
```
From the summary table we can see that P-values for attitude is statistically significant and stra and deep variables' P-values are not. Therefore variables stra and deep are excluded.

```{r}
my_model <- lm(points ~ attitude, data = table1)
summary(my_model)
```

Now P-value is 4.12e-09 (<< 0.05). **Residuals** are the difference between actual values and values that the model predicted. Residuals should be distributed symmetrically on the both sides of zero and the mean line should be quite horizontal. 

From the Coefficients part of the summary we can see that **Intercept Estimate** is 11.63715. That means that the average of the predicted target variable (points) in our dataset is 11.63715 when explanatory variables are zero. The next row is our explanatory variable attitude. The **coefficient estimate** for attitude is 0.35255 and it tells us that when attitude increases one unit, the points will increase by 0.35255 units. **Standard error** for this is 0.05674 and it tells us that the coefficient estimate can vary by 0.05674 units. The **t-value** is used to decide whether to reject null hypothesis or not. It measures how many standard deviation units is the coefficient estimate from zero. If the t-value is small we cannot reject null hypothesis. The **PR(>|t|)** is the propability of finding any observation larger than the t-value. Small P-value indicates that there is a real association between the explanatory variables and the target variable and the coefficient estimate is not due to chance.The **residual standard error** tells how much on average do predicted values deviate from the regression line. Degrees of freedom is basically the number of observations that was used to calculate the residual standard error. The **multiple R-squared** is always between 0 and 1 and it measures how well explanatory variables explain the target variable. In this model R-squred is 0.1906 and it means that 19 % of the changes in the target variable (points) can be explained by the explanatory variables (attitude). **Adjusted R-square** adjusts the R-square when multiple explanatory variables are used (otherwise R-square would always increase when more variables are used). The **F-statistic** measures the association between explanatory variables and target variable. The larger the F-statistic is the better but it should always be larger than 1 to make the results statistically significant.

In conclusion in this model residuals are symmetrically distributed, t-value for coefficient estimate (attitude) is over 6 (> 0) and P-value for coefficient estimate is 4.12e-09 (<<< 0.05). F-statistic is 38.61 (> 1) and P-value for that is 4.119e-09. So all in all we can reject null hypothesis ("There is no association between attitude towards statistics and total points of the query.").

### Diagnostic plots 

```{r}
plot(my_model)
```

**Residual vs Fitted plot** shows a horizontal line which means that residuals are symmetrically distributed. **Normal Q-Q plot** has a quite straight line so residuals are normally distributed and not skewed. **Scale location plot** shows a horizontal line meaning that residuals are spread equally. There are no outliers on the **Residuals vs Leverage plot** so there are no "influential" cases meaning that there are no residuals that could skew the results alone. 


